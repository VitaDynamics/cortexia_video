[logging]
level = "INFO"
file = "app.log"

[model_settings]
object_listing_model = "Qwen/Qwen2.5-VL-7B-Instruct" # Best Usage instead using a VLM model
object_detection_model = "IDEA-Research/grounding-dino-base" # or use YoLo-World
segmentation_model = "facebook/sam-vit-huge"
description_model = "nvidia/DAM-3B-Self-Contained"
clip_feature_model = "PE-Core-G14-448" # Current SOTA CLIP like Vision Encoder.
clip_feature_model_identifier = "clip_pe"
image_captioning_model = "vikhyatk/moondream2"

[object_listing_settings]
task_prompt = "Analyze the following image and list the labels of objects in the image. Any label is acceptable."

[detection_settings]
box_threshold = 0.3
text_threshold = 0.3


[description_settings]
temperature = 0.2
top_p = 0.5
num_beams = 1
max_tokens = 512


[processing] # FIXME: TO use scripts, we dropoff the usage of processing wiht Orchestration. May need update.
default_mode = "list | detect | segment | describe | extract_scene | extract_object"
input_video_path = "sample_data/"
output_directory = "output/" # Output directory for the processed video for visualization. Only works if visualization is enabled.
frame_interval = 30
image_format = "jpg"

[visualization]
enabled = true
annotated_image_format = "jpg"
contour_enabled = true  # Enable contour visualization for segmentation
contour_thickness = 3   # Thickness of contour lines
description_viz_enabled = false  # Enable visualization with descriptions 