[logging]
level = "INFO"
file = "app.log"

[model_settings]
object_listing_model = "Qwen/Qwen2.5-VL-3B-Instruct" # Best Usage instead using a VLM model
object_detection_model = "IDEA-Research/grounding-dino-base" # or use YoLo-World
segmentation_model = "facebook/sam-vit-huge" 
description_model = "nvidia/DAM-3B-Self-Contained"
clip_feature_model = "PE-Core-B16-224" # Current SOTA CLIP like Vision Encoder.
clip_feature_model_identifier = "clip_pe"
image_captioning_model = "vikhyatk/moondream2"

[detection_settings]
box_threshold = 0.3
text_threshold = 0.3


[description_settings]
temperature = 0.2
top_p = 0.5
num_beams = 1
max_tokens = 512


[processing]
default_mode = "list | detect | segment  | extract_scene | extract_object"
input_video_path = "sample_data/"
output_directory = "output/" # Output directory for the processed video for visualization. Only works if visualization is enabled.
frame_interval = 50
batch_size = 2
image_format = "jpg"

[visualization]
enabled = true
annotated_image_format = "jpg"
contour_enabled = true  # Enable contour visualization for segmentation
contour_thickness = 3   # Thickness of contour lines
description_viz_enabled = false  # Enable visualization with descriptions 