[logging]
level = "INFO"
file = "app.log"

[model_settings]
object_listing_model = "recognize_anything/ram" # Best Usage instead using a VLM model
object_detection_model = "IDEA-Research/grounding-dino-tiny" # or use YoLo-World
segmentation_model = "facebook/sam-vit-base" 
description_model = "nvidia/DAM-3B-Self-Contained"
clip_feature_model = "PE-Core-B16-224" # Current SOTA CLIP like Vision Encoder.
clip_feature_model_identifier = "clip_pe"
image_captioning_model = "vikhyatk/moondream2"


[detection_settings]
box_threshold = 0.3
text_threshold = 0.3


[description_settings]
temperature = 0.2
top_p = 0.5
num_beams = 1
max_tokens = 512


[processing]
default_mode = "list detect segment describe extract_features"
input_video_path = "sample_data/"
output_directory = "output/"
frame_interval = 100
image_format = "jpg"

[visualization]
enabled = false
annotated_image_format = "jpg"
contour_enabled = false  # Enable contour visualization for segmentation
contour_thickness = 3   # Thickness of contour lines
description_viz_enabled = false  # Enable visualization with descriptions 