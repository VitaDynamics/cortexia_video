{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e6dc2d",
   "metadata": {},
   "source": [
    "# Use Cortexia with Lance Dataset\n",
    "\n",
    "Example: Use Cortexia with a Lance table.\n",
    "\n",
    "This example shows how to:\n",
    "- Read images stored as bytes from a Lance dataset column\n",
    "- Run features: Caption, Listing\n",
    "- Use Listing tags as prompts for Detection, then run Segmentation\n",
    "- Save annotated results to a new Lance table (or Parquet fallback)\n",
    "\n",
    "Assumptions:\n",
    "- The Lance dataset at `dummys/lance_data/all_in_one.lance` exists.\n",
    "- Image bytes are stored in the `camera_left` column (e.g., JPEG/PNG bytes).\n",
    "- The table has no annotations; other columns (video/frame ids) are optional.\n",
    "\n",
    "If your columns differ, set the env vars or change the defaults below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e29015",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67a4b1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07f754ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import deps \n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import json\n",
    "import datetime\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import pyarrow as pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20238327",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HOME\"]=\"/vita-vepfs-data/fileset1/model/heng.li/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make local package importable when running from cookbook/\n",
    "parent_path = str(Path.cwd().parent)\n",
    "if parent_path not in sys.path:\n",
    "    sys.path.append(parent_path)\n",
    "REPO_ROOT = parent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "189f1417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cotexia related thing \n",
    "import cortexia\n",
    "from cortexia.data.models.video import VideoFramePacket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c311cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Config\n",
    "# ----------------------------------------------------------------------------\n",
    "_repo_path = Path(REPO_ROOT)  # convert str to Path for safe joining\n",
    "\n",
    "DATASET_PATH = os.environ.get(\n",
    "    \"LANCE_DATASET\",\n",
    "    str(_repo_path / \"dummys\" / \"lance_data\" / \"all_in_one.lance\")\n",
    ")\n",
    "\n",
    "# Column names (customize as needed)\n",
    "IMAGE_COL = os.environ.get(\"LANCE_IMAGE_COL\", \"camera_left\")\n",
    "VIDEO_ID_COL = os.environ.get(\"LANCE_VIDEO_ID_COL\", None)   # e.g., \"video_id\" if present\n",
    "FRAME_NUM_COL = os.environ.get(\"LANCE_FRAME_NUM_COL\", None) # e.g., \"frame_number\" if present\n",
    "TIMESTAMP_COL = os.environ.get(\"LANCE_TIMESTAMP_COL\", None) # optional ms/seconds; default to index/30\n",
    "\n",
    "# Output path for annotated table\n",
    "OUTPUT_LANCE = os.environ.get(\n",
    "    \"LANCE_OUTPUT\",\n",
    "    str(_repo_path / \"dummys\" / \"lance_data\" / \"all_in_one_annotated.lance\")\n",
    ")\n",
    "OUTPUT_PARQUET = os.environ.get(\n",
    "    \"PARQUET_OUTPUT\",\n",
    "    str(_repo_path / \"dummys\" / \"lance_data\" / \"all_in_one_annotated.parquet\")\n",
    ")\n",
    "# Limit rows for demo (e.g., 8). Use 0 or unset to disable limiting.\n",
    "ROW_LIMIT = int(os.environ.get(\"LANCE_ROW_LIMIT\", \"8\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf644ff7",
   "metadata": {},
   "source": [
    "## Helpers to work with lance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c06ec613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_lance_table(dataset_path: str) -> pa.Table:\n",
    "    \"\"\"Load the entire Lance dataset into a PyArrow Table.\n",
    "\n",
    "    For simplicity of the cookbook example we load all rows. For large datasets,\n",
    "    adapt to stream batches or filter rows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import lance\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Lance Python package is required for this example. Install 'lance'.\"\n",
    "        ) from e\n",
    "\n",
    "    ds = lance.dataset(dataset_path)\n",
    "    # Convert to Arrow table (small demo dataset assumed)\n",
    "    tbl = ds.to_table()\n",
    "    return tbl\n",
    "\n",
    "\n",
    "def decode_image_from_bytes(b: bytes) -> np.ndarray:\n",
    "    \"\"\"Decode image bytes (e.g., JPEG/PNG) into an RGB numpy array.\"\"\"\n",
    "    with Image.open(io.BytesIO(b)) as im:\n",
    "        im = im.convert(\"RGB\")\n",
    "        return np.array(im)\n",
    "\n",
    "\n",
    "def build_video_frame_packet(row: pa.Table, row_idx: int, full_table: Optional[pa.Table] = None) -> VideoFramePacket:\n",
    "    \"\"\"Construct a VideoFramePacket from a 1-row Arrow table slice.\"\"\"\n",
    "    # Image\n",
    "    img_val = row[IMAGE_COL][0]\n",
    "    if hasattr(img_val, \"as_py\"):\n",
    "        img_val = img_val.as_py()\n",
    "    frame_np = decode_image_from_bytes(img_val)\n",
    "\n",
    "    # Video/frame/timestamp\n",
    "    if VIDEO_ID_COL and VIDEO_ID_COL in row.column_names:\n",
    "        vid = str(row[VIDEO_ID_COL][0])\n",
    "    else:\n",
    "        vid = \"lance_demo\"\n",
    "\n",
    "    if FRAME_NUM_COL and FRAME_NUM_COL in row.column_names:\n",
    "        frame_no = int(row[FRAME_NUM_COL][0])\n",
    "    else:\n",
    "        frame_no = int(row_idx)\n",
    "\n",
    "    if TIMESTAMP_COL and TIMESTAMP_COL in row.column_names:\n",
    "        ts_val = row[TIMESTAMP_COL][0]\n",
    "        if hasattr(ts_val, \"as_py\"):\n",
    "            ts_val = ts_val.as_py()\n",
    "        # Interpret as seconds if float, ms if int\n",
    "        if isinstance(ts_val, float):\n",
    "            ts = datetime.timedelta(seconds=ts_val)\n",
    "        elif isinstance(ts_val, int):\n",
    "            ts = datetime.timedelta(milliseconds=ts_val)\n",
    "        else:\n",
    "            ts = datetime.timedelta(seconds=frame_no / 30.0)\n",
    "    else:\n",
    "        ts = datetime.timedelta(seconds=frame_no / 30.0)\n",
    "\n",
    "    # Build trajectory data using current + next 6 frames (7 points total)\n",
    "    trajectory_points = []\n",
    "    if full_table is not None:\n",
    "        # Import trajectory models\n",
    "        from cortexia.data.models.video import TrajectoryPoint\n",
    "        \n",
    "        # Collect trajectory points for current frame and next 6 frames\n",
    "        for j in range(7):  # 0 to 6\n",
    "            frame_idx = row_idx + j\n",
    "            if frame_idx >= len(full_table):\n",
    "                # Not enough future frames, break early\n",
    "                break\n",
    "                \n",
    "            # Get odo data for this frame\n",
    "            if 'odo' in full_table.column_names:\n",
    "                odo_data = full_table['odo'][frame_idx]\n",
    "                if hasattr(odo_data, 'as_py'):\n",
    "                    odo_data = odo_data.as_py()\n",
    "                \n",
    "                # odo_data should contain 7 numbers: x, y, z, qx, qy, qz, qw\n",
    "                if isinstance(odo_data, (list, tuple)) and len(odo_data) >= 7:\n",
    "                    x, y, z, qx, qy, qz, qw = odo_data[:7]\n",
    "                    traj_point = TrajectoryPoint(x=x, y=y, z=z, qx=qx, qy=qy, qz=qz, qw=qw)\n",
    "                else:\n",
    "                    # Create default trajectory point if invalid odo data\n",
    "                    traj_point = TrajectoryPoint(x=frame_idx*0.1, y=0.0, z=0.0, qx=0.0, qy=0.0, qz=0.0, qw=1.0)\n",
    "            else:\n",
    "                # No odo column, create simulated trajectory data\n",
    "                traj_point = TrajectoryPoint(x=frame_idx*0.1, y=0.0, z=0.0, qx=0.0, qy=0.0, qz=0.0, qw=1.0)\n",
    "            \n",
    "            trajectory_points.append(traj_point)\n",
    "\n",
    "    return VideoFramePacket(\n",
    "        frame_data=frame_np,\n",
    "        frame_number=frame_no,\n",
    "        timestamp=ts,\n",
    "        source_video_id=vid,\n",
    "        additional_metadata={},\n",
    "        trajecotry=trajectory_points,  # Note: field name has typo\n",
    "        current_traj_index=0  # Current frame is always at index 0\n",
    "    )\n",
    "\n",
    "\n",
    "def make_loader(table: pa.Table):\n",
    "    \"\"\"Create a BatchProcessor-compatible loader over a fixed Arrow table.\"\"\"\n",
    "    def load_func(indices: List[int]) -> List[VideoFramePacket]:\n",
    "        # Take a subset table by row indices and convert to packets\n",
    "        sub = table.take(pa.array(indices))\n",
    "        frames: List[VideoFramePacket] = []\n",
    "        for pos, row_idx in enumerate(indices):\n",
    "            one = sub.slice(pos, 1)\n",
    "            frames.append(build_video_frame_packet(one, row_idx))\n",
    "        return frames\n",
    "\n",
    "    return load_func\n",
    "\n",
    "def results_to_struct_array(results):\n",
    "    if not results:\n",
    "        return pa.array([], type=pa.null())\n",
    "    first_struct = results[0].to_pyarrow_struct()\n",
    "    struct_type = first_struct.type\n",
    "    names = struct_type.names\n",
    "    dicts = []\n",
    "    for r in results:\n",
    "        s = r.to_pyarrow_struct()\n",
    "        scalar = s[0]\n",
    "        row = {}\n",
    "        for name in names:\n",
    "            try:\n",
    "                val = scalar[name]\n",
    "                row[name] = val.as_py() if val is not None else None\n",
    "            except KeyError:\n",
    "                row[name] = None\n",
    "        dicts.append(row)\n",
    "    return pa.array(dicts, type=struct_type)\n",
    "\n",
    "\n",
    "def limit_table(table: pa.Table, limit: int | None) -> pa.Table:\n",
    "    \"\"\"Return a slice of the table limited to `limit` rows (if > 0).\"\"\"\n",
    "    try:\n",
    "        if limit is None or int(limit) <= 0:\n",
    "            return table\n",
    "        limit = min(int(limit), len(table))\n",
    "        return table.slice(0, limit)\n",
    "    except Exception:\n",
    "        return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd3382",
   "metadata": {},
   "source": [
    "## Create Features and get Lance Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ee4fec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Lance table: /home/heng.li/repo/cortexia_video/dummys/lance_data/all_in_one.lance\n",
      "Rows: 8; image column: 'camera_left'\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Lance table:\", DATASET_PATH)\n",
    "table = load_lance_table(DATASET_PATH)\n",
    "# We Limit Table for demo runs\n",
    "table = limit_table(table, ROW_LIMIT)\n",
    "schema_cols = set(table.column_names)\n",
    "if IMAGE_COL not in schema_cols:\n",
    "    raise ValueError(f\"Column '{IMAGE_COL}' not found. Available: {sorted(schema_cols)}\")\n",
    "\n",
    "n_rows = len(table)\n",
    "print(f\"Rows: {n_rows}; image column: '{IMAGE_COL}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "909567d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'camera_left',\n",
       " 'camera_right',\n",
       " 'command',\n",
       " 'day',\n",
       " 'device',\n",
       " 'icp',\n",
       " 'joy',\n",
       " 'odo',\n",
       " 'pointcloud',\n",
       " 'status',\n",
       " 'timestamp',\n",
       " 'trigger',\n",
       " 'uwb'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54a91732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.ListScalar: [13.825, -2.221, 0.31, 0.022, -0.02, -0.015, -0.999]>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[\"odo\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb0c3dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.StringScalar: None>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[\"status\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "838e9c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caption',\n",
       " 'depth',\n",
       " 'description',\n",
       " 'detection',\n",
       " 'listing',\n",
       " 'segmentation',\n",
       " 'feature_extraction',\n",
       " 'trajectory']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cortexia.list_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2ed9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices are row numbers\n",
    "indices = list(range(n_rows))\n",
    "load_func = make_loader(table)\n",
    "\n",
    "# Features\n",
    "caption = cortexia.create_feature(\"caption\")\n",
    "listing = cortexia.create_feature(\"listing\")\n",
    "detection = cortexia.create_feature(\"detection\")\n",
    "segmentation = cortexia.create_feature(\"segmentation\")\n",
    "trajectory = cortexia.create_feature(\"trajectory\")\n",
    "\n",
    "# Materialize frames once for chaining and attachment\n",
    "frames = load_func(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25668092",
   "metadata": {},
   "source": [
    "**Why do we use add_annotation_result** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d6a684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Caption via BatchProcessor and attaching to frames...\n",
      "Processing batch 1/2 (4 items)\n",
      "Loaded 4 objects\n",
      "Running inference on 4 objects\n",
      "Generated 4 results\n",
      "Processing batch 2/2 (4 items)\n",
      "Loaded 4 objects\n",
      "Running inference on 4 objects\n",
      "Generated 4 results\n"
     ]
    }
   ],
   "source": [
    "from cortexia.data.io.batch_processor import BatchProcessor\n",
    "print(\"Running Caption via BatchProcessor and attaching to frames...\")\n",
    "frames_map = {idx: frames[idx] for idx in indices}\n",
    "bp = BatchProcessor(batch_size=4)\n",
    "bp.load_indices(indices)\n",
    "\n",
    "def bp_load(batch_indices: List[int]) -> List[VideoFramePacket]:\n",
    "    return [frames_map[i] for i in batch_indices]\n",
    "\n",
    "def bp_infer(fr_batch: List[VideoFramePacket], batch_indices: List[int]):\n",
    "    return caption.process_batch(fr_batch)\n",
    "\n",
    "def bp_save(idx: int, result):\n",
    "    frames_map[idx].add_annotation_result(result)\n",
    "\n",
    "# Process in batches and attach directly using save_func\n",
    "_ = bp.process_batch(load_func=bp_load, inference_func=bp_infer, save_func=bp_save, filter_func=None)\n",
    "\n",
    "# Collect attached caption results back from frames for later writing\n",
    "cap_results = []\n",
    "for f in frames:\n",
    "    if f.annotations and 'CaptionResult' in f.annotations:\n",
    "        cap_results.append(f.annotations['CaptionResult'])\n",
    "    else:\n",
    "        from cortexia.data.models.result.caption_result import CaptionResult\n",
    "        cap_results.append(CaptionResult(caption=\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95201d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Listing via BatchProcessor and attaching to frames...\n",
      "Processing batch 1/2 (4 items)\n",
      "Loaded 4 objects\n",
      "Running inference on 4 objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.97s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 results\n",
      "Processing batch 2/2 (4 items)\n",
      "Loaded 4 objects\n",
      "Running inference on 4 objects\n",
      "Generated 4 results\n"
     ]
    }
   ],
   "source": [
    "# 2) Listing (BatchProcessor chain; attach + set prompts for detection)\n",
    "print(\"Running Listing via BatchProcessor and attaching to frames...\")\n",
    "list_results_map = {}\n",
    "bp2 = BatchProcessor(batch_size=4)\n",
    "bp2.load_indices(indices)\n",
    "\n",
    "def bp2_load(batch_indices: List[int]) -> List[VideoFramePacket]:\n",
    "    return [frames_map[i] for i in batch_indices]\n",
    "\n",
    "def bp2_infer(fr_batch: List[VideoFramePacket], batch_indices: List[int]):\n",
    "    return listing.process_batch(fr_batch)\n",
    "\n",
    "def bp2_save(idx: int, result):\n",
    "    f = frames_map[idx]\n",
    "    f.add_annotation_result(result)\n",
    "    # Also provide prompts for detection via metadata\n",
    "    f.additional_metadata[\"lister_results\"] = list(getattr(result, 'tags', []) or [])\n",
    "    list_results_map[idx] = result\n",
    "\n",
    "_ = bp2.process_batch(load_func=bp2_load, inference_func=bp2_infer, save_func=bp2_save, filter_func=None)\n",
    "list_results = [list_results_map[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44bf8366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person walking', 'red box', 'sidewalk']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_results[0].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a8e9394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Detection (prompted by Listing tags) and attaching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heng.li/repo/cortexia_video/.venv/lib/python3.10/site-packages/transformers/models/grounding_dino/processing_grounding_dino.py:100: FutureWarning: The key `labels` is will return integer ids in `GroundingDinoProcessor.post_process_grounded_object_detection` output since v4.51.0. Use `text_labels` instead to retrieve string object names.\n",
      "  warnings.warn(self.message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 3) Detection\n",
    "print(\"Running Detection (prompted by Listing tags) and attaching...\")\n",
    "det_results = detection.process_batch(frames)\n",
    "for f, r in zip(frames, det_results):\n",
    "    f.add_annotation_result(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02cbc418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<cortexia.data.models.result.detection_result.SingleDetection at 0x7f1f9c7dfcd0>,\n",
       " <cortexia.data.models.result.detection_result.SingleDetection at 0x7f1f1f7da200>,\n",
       " <cortexia.data.models.result.detection_result.SingleDetection at 0x7f1f1c39add0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_results[0].detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e3aec2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Segmentation (using Detection boxes) and attaching...\n"
     ]
    }
   ],
   "source": [
    "# 4) Segmentation\n",
    "print(\"Running Segmentation (using Detection boxes) and attaching...\")\n",
    "seg_results = segmentation.process_batch(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74e77a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<cortexia.data.models.result.segmentation_result.SingleSegmentation object at 0x7f1f1c26a470>\n"
     ]
    }
   ],
   "source": [
    "print(seg_results[0].segmentations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d6d12c",
   "metadata": {},
   "source": [
    "**5) Trajectory Analysis - Annotate Action States**\n",
    "\n",
    "Now let's use the trajectory feature to analyze movement states.\n",
    "We'll simulate trajectory data from the 'odo' column which contains \n",
    "7 numbers: x, y, z, qx, qy, qz, qw representing position and orientation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df7651b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Trajectory Analysis to annotate action states...\n",
      "Trajectory Analysis Results:\n",
      "Frame 0:\n",
      "  Current state: forward\n",
      "  State distribution: {'forward': 4, 'backward': 0, 'stop': 3, 'forward_left': 0, 'forward_right': 0, 'backward_left': 0, 'backward_right': 0}\n",
      "  Position: (13.82, -2.22, 0.31)\n",
      "  Orientation: yaw=1.7°\n",
      "\n",
      "Frame 1:\n",
      "  Current state: forward\n",
      "  State distribution: {'forward': 4, 'backward': 0, 'stop': 3, 'forward_left': 0, 'forward_right': 0, 'backward_left': 0, 'backward_right': 0}\n",
      "  Position: (13.96, -2.22, 0.30)\n",
      "  Orientation: yaw=1.4°\n",
      "\n",
      "Frame 2:\n",
      "  Current state: forward\n",
      "  State distribution: {'forward': 3, 'backward': 0, 'stop': 3, 'forward_left': 0, 'forward_right': 0, 'backward_left': 0, 'backward_right': 0}\n",
      "  Position: (14.09, -2.22, 0.29)\n",
      "  Orientation: yaw=1.6°\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\"Running Trajectory Analysis to annotate action states...\")\n",
    "\n",
    "# Import trajectory models at function level to avoid import issues\n",
    "from cortexia.data.models.video import TrajectoryPoint\n",
    "\n",
    "# Build trajectory data for each frame using current + next 6 frames\n",
    "for i, frame in enumerate(frames):\n",
    "    trajectory_points = []\n",
    "    \n",
    "    # Collect trajectory points for current frame and next 6 frames (7 points total)\n",
    "    for j in range(7):  # 0 to 6\n",
    "        frame_idx = i + j\n",
    "        if frame_idx >= len(table):\n",
    "            # Not enough future frames, break early\n",
    "            break\n",
    "            \n",
    "        # Get odo data for this frame\n",
    "        if 'odo' in table.column_names:\n",
    "            odo_data = table['odo'][frame_idx]\n",
    "            if hasattr(odo_data, 'as_py'):\n",
    "                odo_data = odo_data.as_py()\n",
    "            \n",
    "            # odo_data should contain 7 numbers: x, y, z, qx, qy, qz, qw\n",
    "            if isinstance(odo_data, (list, tuple)) and len(odo_data) >= 7:\n",
    "                x, y, z, qx, qy, qz, qw = odo_data[:7]\n",
    "                traj_point = TrajectoryPoint(x=x, y=y, z=z, qx=qx, qy=qy, qz=qz, qw=qw)\n",
    "            else:\n",
    "                # Create default trajectory point if invalid odo data\n",
    "                traj_point = TrajectoryPoint(x=frame_idx*0.1, y=0.0, z=0.0, qx=0.0, qy=0.0, qz=0.0, qw=1.0)\n",
    "        else:\n",
    "            # No odo column, create simulated trajectory data\n",
    "            traj_point = TrajectoryPoint(x=frame_idx*0.1, y=0.0, z=0.0, qx=0.0, qy=0.0, qz=0.0, qw=1.0)\n",
    "        \n",
    "        trajectory_points.append(traj_point)\n",
    "    \n",
    "    # Set trajectory and current index directly on frame (note: typo in field name)\n",
    "    frame.trajecotry = trajectory_points  # Note: field name has typo\n",
    "    frame.current_traj_index = 0  # Current frame is always at index 0\n",
    "\n",
    "# Process trajectory analysis\n",
    "traj_results = trajectory.process_batch(frames)\n",
    "\n",
    "# Attach trajectory results to frames\n",
    "for frame, traj_result in zip(frames, traj_results):\n",
    "    frame.add_annotation_result(traj_result)\n",
    "\n",
    "# %%\n",
    "# Examine trajectory analysis results\n",
    "print(\"Trajectory Analysis Results:\")\n",
    "for i in range(min(3, len(frames))):\n",
    "    traj_result = traj_results[i]\n",
    "    current_state = traj_result.get_current_state()\n",
    "    state_dist = traj_result.state_distribution\n",
    "    \n",
    "    print(f\"Frame {i}:\")\n",
    "    print(f\"  Current state: {current_state}\")\n",
    "    print(f\"  State distribution: {state_dist}\")\n",
    "    \n",
    "    # Get current trajectory point details\n",
    "    if 0 <= traj_result.current_index < len(traj_result.trajectory_points):\n",
    "        point = traj_result.trajectory_points[traj_result.current_index]\n",
    "        print(f\"  Position: ({point.x:.2f}, {point.y:.2f}, {point.z:.2f})\")\n",
    "        print(f\"  Orientation: yaw={math.degrees(point.yaw):.1f}°\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1639c5b",
   "metadata": {},
   "source": [
    "**Lets check some examples** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa155c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0 -> caption: 'The image shows a person walking down a paved pathway in an outdoor setting. The individual is wearing a dark-colored shirt and dark pants, carrying a red shopping bag in their left hand. The path is made of reddish-brown tiles laid in a geometric pattern, and extends into the distance, bordered by green grass and trees. The sky is cloudy and gray, suggesting an overcast day. In the background, additional people can be seen walking or standing, along a raised walkway or bridge that runs parallel to the path. The overall atmosphere appears calm and peaceful, with natural greenery surrounding the pathway.'\n",
      "Row 0 -> tags: ['person walking', 'red box', 'sidewalk']\n",
      "Row 0 -> det: count=3, first: label='person walking', score=0.719, box=[880.4154052734375, 3.776024580001831, 1134.88134765625, 694.2313232421875]\n",
      "Row 0 -> seg: count=3, first: label='person walking', area=101307, mask.shape=(1080, 1920)\n",
      "Row 1 -> caption: 'The image shows a person walking on a paved pathway in a park-like setting. The individual is wearing a dark-colored shirt and dark-colored pants, carrying a red bag with their left hand and appearing to walk away from the camera. The pathway is made of reddish-brown tiles laid in a geometric pattern, and extends into the distance, bordered by green grass and trees. The sky is cloudy and gray, suggesting an overcast day. In the background, additional people can be seen walking or standing, further enhancing the park-like atmosphere. The overall scene conveys a calm, leisurely atmosphere, typical of a public park or recreational area.'\n",
      "Row 1 -> tags: ['person walking', 'red box', 'sidewalk']\n",
      "Row 1 -> det: count=3, first: label='person walking', score=0.669, box=[841.5414428710938, 2.1597790718078613, 1081.86865234375, 676.3736572265625]\n",
      "Row 1 -> seg: count=3, first: label='person walking', area=94871, mask.shape=(1080, 1920)\n"
     ]
    }
   ],
   "source": [
    "for i in range(min(2, n_rows)):\n",
    "    print(f\"Row {i} -> caption: {cap_results[i].caption!r}\")\n",
    "    print(f\"Row {i} -> tags: {list_results[i].tags}\")\n",
    "    # Handle new DetectionResult format with multiple detections\n",
    "    if det_results[i].has_detections:\n",
    "        # Show first detection as example\n",
    "        first_det = det_results[i].detections[0]\n",
    "        print(\n",
    "            f\"Row {i} -> det: count={det_results[i].count}, first: label={first_det.label!r}, score={first_det.score:.3f}, box={first_det.box.xyxy}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Row {i} -> det: count=0, no detections\")\n",
    "    # Handle new SegmentationResult format with multiple segmentations\n",
    "    if seg_results[i].has_segmentations:\n",
    "        # Show first segmentation as example\n",
    "        first_seg = seg_results[i].segmentations[0]\n",
    "        print(\n",
    "            f\"Row {i} -> seg: count={seg_results[i].count}, first: label={first_seg.label!r}, area={first_seg.area}, mask.shape={first_seg.mask.shape}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Row {i} -> seg: count=0, no segmentations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d034f",
   "metadata": {},
   "source": [
    "**Lets write it into a table with annotation result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce69bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_caption_struct = results_to_struct_array(cap_results)\n",
    "col_tags_struct = results_to_struct_array(list_results)\n",
    "col_det_struct = results_to_struct_array(det_results)\n",
    "col_seg_struct = results_to_struct_array(seg_results)\n",
    "\n",
    "annotated = table\n",
    "annotated = annotated.append_column(\"cortexia_caption\", col_caption_struct)\n",
    "annotated = annotated.append_column(\"cortexia_tags\", col_tags_struct)\n",
    "annotated = annotated.append_column(\"cortexia_detection\", col_det_struct)\n",
    "annotated = annotated.append_column(\"cortexia_segmentation\", col_seg_struct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6893c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated Lance dataset written to: /home/heng.li/repo/cortexia_video/dummys/lance_data/all_in_one_annotated.lance\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import lance\n",
    "    out = Path(OUTPUT_LANCE)\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # Overwrite destination if exists by writing a fresh dataset\n",
    "    if out.exists():\n",
    "        # Best-effort cleanup; Lance manages versions but this is a demo\n",
    "        import shutil\n",
    "        shutil.rmtree(out, ignore_errors=True)\n",
    "    lance.write_dataset(annotated, str(out))\n",
    "    print(f\"Annotated Lance dataset written to: {out}\")\n",
    "    wrote = True\n",
    "except Exception as e:\n",
    "    print(f\"Lance write failed or unavailable: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42fc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previewing first 3 rows (selected columns):\n",
      "Row 0: {'cortexia_caption': {'caption': 'The image shows a person walking down a paved pathway in an outdoor setting. The individual is wearing a dark-colored shirt and dark pants, carrying a red shopping bag in their left hand. The path is made of reddish-brown tiles laid in a geometric pattern, and extends into the distance, bordered by green grass and trees. The sky is cloudy and gray, suggesting an overcast day. In the background, additional people can be seen walking or standing, along a raised walkway or bridge that runs parallel to the path. The overall atmosphere appears calm and peaceful, with natural greenery surrounding the pathway.', 'confidence': None, 'model_name': 'vikhyatk/moondream2', 'caption_length': 'long', 'processing_time_ms': None}, 'cortexia_tags': {'tags': ['person walking', 'red box', 'sidewalk'], 'raw_response': 'person walking. red box. sidewalk.', 'confidence_scores': None, 'model_name': 'vikhyatk/moondream2', 'processing_time_ms': None}, 'cortexia_detection': {'detections': ['<cortexia.data.models.result.detection_result.SingleDetection object at 0x7fb5f1b5fcd0>', '<cortexia.data.models.result.detection_result.SingleDetection object at 0x7fb5f0b180a0>', '<cortexia.data.models.result.detection_result.SingleDetection object at 0x7fb5582097e0>'], 'model_name': 'IDEA-Research/grounding-dino-base', 'processing_time_ms': None}, 'cortexia_segmentation': {'segmentations': ['<cortexia.data.models.result.segmentation_result.SingleSegmentation object at 0x7fb54ddec220>', '<cortexia.data.models.result.segmentation_result.SingleSegmentation object at 0x7fb54ddec520>', '<cortexia.data.models.result.segmentation_result.SingleSegmentation object at 0x7fb54ddec1f0>'], 'model_name': None, 'processing_time_ms': None}}\n",
      "Row 1: {'cortexia_caption': {'caption': 'The image shows a person walking on a paved pathway in a park-like setting. The individual is wearing a dark-colored shirt and dark-colored pants, carrying a red bag with their left hand and appearing to walk away from the camera. The pathway is made of reddish-brown tiles laid in a geometric pattern, and extends into the distance, bordered by green grass and trees. The sky is cloudy and gray, suggesting an overcast day. In the background, additional people can be seen walking or standing, further enhancing the park-like atmosphere. The overall scene conveys a calm, leisurely atmosphere, typical of a public park or recreational area.', 'confidence': None, 'model_name': 'vikhyatk/moondream2', 'caption_length': 'long', 'processing_time_ms': None}, 'cortexia_tags': {'tags': ['person walking', 'red box', 'sidewalk'], 'raw_response': 'person walking. red box. sidewalk.', 'confidence_scores': None, 'model_name': 'vikhyatk/moondream2', 'processing_time_ms': None}, 'cortexia_detection': {'detections': ['<cortexia.data.models.result.detection_result.SingleDetection object at 0x7fb5582099c0>', '<cortexia.data.models.result.detection_result.SingleDetection object at 0x7fb558209750>', '<cortexia.data.models.result.detection_result.SingleDetection object at 0x7fb5582098a0>'], 'model_name': 'IDEA-Research/grounding-dino-base', 'processing_time_ms': None}, 'cortexia_segmentation': {'segmentations': ['<cortexia.data.models.result.segmentation_result.SingleSegmentation object at 0x7fb54ddec3a0>', '<cortexia.data.models.result.segmentation_result.SingleSegmentation object at 0x7fb54ddec3d0>', '<cortexia.data.models.result.segmentation_result.SingleSegmentation object at 0x7fb54ddec430>'], 'model_name': None, 'processing_time_ms': None}}\n",
      "Row 2: {'cortexia_caption': {'caption': 'The image shows a person walking down a paved pathway in a park-like setting. The individual is wearing a dark-colored shirt and dark pants, carrying a red shopping bag in their left hand. The path is made of reddish-brown tiles laid in a geometric pattern, and extends into the distance, bordered by green grass and trees. The sky is cloudy and gray, suggesting an overcast day. In the background, additional people can be seen walking or standing, enjoying the park atmosphere. The overall atmosphere of the image is calm and peaceful, typical of a leisurely outdoor walk in a park environment.', 'confidence': None, 'model_name': 'vikhyatk/moondream2', 'caption_length': 'long', 'processing_time_ms': None}, 'cortexia_tags': {'tags': ['person walking', 'red bag with Chinese characters', 'sidewalk', 'trees', 'cloudy sky'], 'raw_response': 'person walking. red bag with Chinese characters. sidewalk. trees. cloudy sky.', 'confidence_scores': None, 'model_name': 'vikhyatk/moondream2', 'processing_time_ms': None}, 'cortexia_detection': {'detections': ['<cortexia.data.models.result.detection_result.SingleDetection object at 0x7fb5582095d0>', '<cortexia.data.models.result.detection_result.SingleDetection object at 0x7fb5582094b0>', '<cortexia.data.models.result.detection_result.SingleDetection object at 0x7fb558209000>', '<cortexia.data.models.result.detection_result.SingleDetection object at 0x7fb558209450>', '<cortexia.data.models.result.detection_result.SingleDetection object at 0x7fb558209090>', '<cortexia.data.models.result.detection_result.SingleDetection object at 0x7fb558208ee0>'], 'model_name': 'IDEA-Research/grounding-dino-base', 'processing_time_ms': None}, 'cortexia_segmentation': {'segmentations': ['<cortexia.data.models.result.segmentation_result.SingleSegmentation object at 0x7fb54ddec250>', '<cortexia.data.models.result.segmentation_result.SingleSegmentation object at 0x7fb54ddec280>', '<cortexia.data.models.result.segmentation_result.SingleSegmentation object at 0x7fb54ddec580>', '<cortexia.data.models.result.segmentation_result.SingleSegmentation object at 0x7fb54ddec700>', '<cortexia.data.models.result.segmentation_result.SingleSegmentation object at 0x7fb54ddec790>', '<cortexia.data.models.result.segmentation_result.SingleSegmentation object at 0x7fb54ddec2b0>'], 'model_name': None, 'processing_time_ms': None}}\n"
     ]
    }
   ],
   "source": [
    "# Take a quick preview for this table\n",
    "def preview_rows(tbl: pa.Table, k: int = 3):\n",
    "    print(\"Previewing first\", min(k, len(tbl)), \"rows (selected columns):\")\n",
    "    cols_to_show = [\n",
    "        c for c in [\n",
    "            VIDEO_ID_COL or None,\n",
    "            FRAME_NUM_COL or None,\n",
    "            \"cortexia_caption\",\n",
    "            \"cortexia_tags\",\n",
    "            \"cortexia_detection\",\n",
    "            \"cortexia_segmentation\",\n",
    "        ] if c and c in tbl.column_names\n",
    "    ]\n",
    "    for i in range(min(k, len(tbl))):\n",
    "        row = tbl.slice(i, 1)\n",
    "        summary = {}\n",
    "        for c in cols_to_show:\n",
    "            cell = row[c][0]\n",
    "            try:\n",
    "                summary[c] = cell.as_py()\n",
    "            except Exception:\n",
    "                summary[c] = str(cell)\n",
    "        print(f\"Row {i}:\", summary)\n",
    "\n",
    "preview_rows(annotated, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d47d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
