{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "340a627b",
   "metadata": {},
   "source": [
    "# Use Cortexia SDK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02ebf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# append cortexia to system PATH.\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "parent_path = str(Path.cwd().parent)\n",
    "if parent_path not in sys.path:\n",
    "    sys.path.append(parent_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdced0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cortexia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5d355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heng.li/repo/cortexia_video/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available features for annotation:\n",
      "['caption', 'depth', 'description', 'listing', 'segmentation', 'feature_extraction']\n",
      "Available gates (for filter and annotation):\n",
      "['blur', 'clip', 'entropy', 'grid', 'hash']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List all features for annotation\n",
    "features = cortexia.list_features()\n",
    "print(\"Available features for annotation:\")\n",
    "print(features)\n",
    "\n",
    "# List all gates (for filter and gates, works as annotations)\n",
    "gates = cortexia.list_gates()\n",
    "print(\"Available gates (for filter and annotation):\")\n",
    "print(gates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6ee89",
   "metadata": {},
   "source": [
    "## Create feature \n",
    "\n",
    "- Use feature name above to create feature for annotation usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "903b8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_feature = cortexia.create_feature(\"caption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f72ee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cortexia.data.models.result.caption_result.CaptionResult'>\n"
     ]
    }
   ],
   "source": [
    "# Each feature have its own output schema for output checking.\n",
    "print(caption_feature.output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cff88da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# We use lazy init for each feature and module. Module only init when actually be used. \n",
    "print(caption_feature.is_ready())\n",
    "\n",
    "# Or we can use _initialize()\n",
    "caption_feature._initialize()\n",
    "\n",
    "# Then we check again \n",
    "print(caption_feature.is_ready())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8de282",
   "metadata": {},
   "source": [
    "## Single Feature Inference\n",
    "\n",
    "Below we show how to use a single feature (e.g. caption) on one or multiple `VideoFramePacket` objects manually without the batch helper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25082abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: CaptionResult\n",
      "=====================\n",
      "\n",
      "  caption: \"The image displays a close-up view of an abstract pattern composed entirely of small, densely packed dots in various colors, including shades of red, pink, blue, green, yellow, purple, and white. The dots appear randomly distributed, creating a textured and slightly blurry appearance. The overall effect is reminiscent of noise or static, with no discernible pattern or structure. The colors are vibrant yet muted, contributing to the image's abstract and somewhat mysterious quality. The dots vary slightly in size and brightness, adding to the visual complexity and depth of the pattern.\"\n",
      "  confidence: None\n",
      "  model_name: 'vikhyatk/moondream2'\n",
      "  caption_length: 'long'\n",
      "  processing_time_ms: None\n",
      "Frame annotations keys: ['CaptionResult']\n"
     ]
    }
   ],
   "source": [
    "import datetime, numpy as np\n",
    "from cortexia.data.models.video import VideoFramePacket\n",
    "\n",
    "# Create a dummy frame\n",
    "frame = VideoFramePacket(\n",
    "    frame_data=np.random.randint(0,255,(224,224,3),dtype=np.uint8),\n",
    "    frame_number=0,\n",
    "    timestamp=datetime.timedelta(seconds=0),\n",
    "    source_video_id=\"demo_video\"\n",
    ")\n",
    "\n",
    "# Use an already created caption_feature (from earlier cell)\n",
    "if not caption_feature.is_ready():\n",
    "    caption_feature._initialize()\n",
    "\n",
    "single_result = caption_feature.process_batch([frame])[0]\n",
    "print(single_result)\n",
    "\n",
    "# Attach annotation back to frame (optional for later chaining)\n",
    "frame.add_annotation_result(single_result)\n",
    "print(\"Frame annotations keys:\", list(frame.annotations.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f43ae",
   "metadata": {},
   "source": [
    "## Gate Usage\n",
    "\n",
    "Gates compute measurements (e.g., blur, entropy) rather than semantic annotations. They also use `process_batch`.\n",
    "We'll create a gate instance and run it on the same frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ae72162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema: GateResult\n",
      "==================\n",
      "\n",
      "  gate_name: 'blur_gate'\n",
      "  score: 47997.69007365398\n",
      "  vector: None\n",
      "  threshold: None\n",
      "  metadata: {metric: 'variance_of_laplacian', frame_number: 0, timestamp: 0.0, ...} (len=4)\n",
      "  processing_time_ms: None\n",
      "Updated annotations keys: ['CaptionResult', 'GateResult']\n"
     ]
    }
   ],
   "source": [
    "# Example: use a blur gate (replace 'blur' with an available gate name from list_gates())\n",
    "blur_gate_cls = cortexia.get_gate(\"blur\")  # get class\n",
    "blur_gate = blur_gate_cls()                 # instantiate\n",
    "\n",
    "blur_result = blur_gate.process_batch([frame])[0]\n",
    "print(blur_result)\n",
    "frame.add_annotation_result(blur_result)\n",
    "print(\"Updated annotations keys:\", list(frame.annotations.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc13695",
   "metadata": {},
   "source": [
    "## Batch Processing with `process_batch_with_processor`\n",
    "\n",
    "The helper wraps `BatchProcessor` to load frames lazily. Provide:\n",
    "- indices: list of references (here we'll simulate simple integers)\n",
    "- load_func: given a sublist of indices returns a list of `VideoFramePacket`\n",
    "- processor: a feature or gate instance (must implement `process_batch`)\n",
    "\n",
    "We'll demo with the caption feature over 8 dummy frames in batches of 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c457de3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/1 (4 items)\n",
      "Loaded 4 objects\n",
      "Running inference on 4 objects\n",
      "Generated 4 results\n",
      "Collected 4 results (expected 4 even indices)\n",
      "Schema: CaptionResult\n",
      "=====================\n",
      "\n",
      "  caption: 'The image displays a textured surface with a multicolored, speckled pattern. The colors are predominantly shades of gray, purple, pink, yellow, and white, creating a subtle and vibrant contrast. The texture appears slightly grainy, resembling static or noise, with no discernible movement or pattern. The overall appearance is abstract and visually interesting due to the random arrangement of colors and shapes. The image is relatively small in size, lacking any discernible features or objects, and focuses solely on the interplay of colors and patterns.'\n",
      "  confidence: None\n",
      "  model_name: 'vikhyatk/moondream2'\n",
      "  caption_length: 'long'\n",
      "  processing_time_ms: None\n",
      "Schema: CaptionResult\n",
      "=====================\n",
      "\n",
      "  caption: 'The image displays a close-up view of a textured surface with a multicolored, speckled pattern. The colors are predominantly shades of blue, green, purple, pink, and yellow, scattered randomly across the surface. The texture appears slightly grainy and blurry, indicative of a low-resolution image or possibly a photograph taken with a very shallow depth of field. The overall appearance conveys a soft, natural, and slightly abstract quality due to the random arrangement of colors and lack of sharp, distinct lines or shapes.'\n",
      "  confidence: None\n",
      "  model_name: 'vikhyatk/moondream2'\n",
      "  caption_length: 'long'\n",
      "  processing_time_ms: None\n"
     ]
    }
   ],
   "source": [
    "from cortexia.api.cortexia import process_batch_with_processor\n",
    "\n",
    "# Simulated indices (e.g., could be frame numbers or file paths)\n",
    "indices = list(range(8))\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "def load_func(batch_indices):\n",
    "    frames = []\n",
    "    for idx in batch_indices:\n",
    "        frames.append(\n",
    "            VideoFramePacket(\n",
    "                frame_data=np.random.randint(0,255,(224,224,3),dtype=np.uint8),\n",
    "                frame_number=idx,\n",
    "                timestamp=datetime.timedelta(seconds=idx/30),\n",
    "                source_video_id=\"demo_video\"\n",
    "            )\n",
    "        )\n",
    "    return frames\n",
    "\n",
    "# Reuse caption_feature\n",
    "results = process_batch_with_processor(\n",
    "    indices=indices,\n",
    "    load_func=load_func,\n",
    "    processor=caption_feature,\n",
    "    batch_size=4,\n",
    "    filter_func=lambda i: i % 2 == 0  # only even indices\n",
    ")\n",
    "\n",
    "print(f\"Collected {len(results)} results (expected 4 even indices)\")\n",
    "for r in results[:2]:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0bc295",
   "metadata": {},
   "source": [
    "### Using the same helper with a Gate\n",
    "\n",
    "You can pass any gate instance to the same helper to compute measurements batch-wise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93449a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/2 (4 items)\n",
      "Loaded 4 objects\n",
      "Running inference on 4 objects\n",
      "Generated 4 results\n",
      "Processing batch 2/2 (4 items)\n",
      "Loaded 4 objects\n",
      "Running inference on 4 objects\n",
      "Generated 4 results\n",
      "Gate results count: 8\n",
      "Schema: GateResult\n",
      "==================\n",
      "\n",
      "  gate_name: 'blur_gate'\n",
      "  score: 47941.43817999441\n",
      "  vector: None\n",
      "  threshold: None\n",
      "  metadata: {metric: 'variance_of_laplacian', frame_number: 0, timestamp: 0.0, ...} (len=4)\n",
      "  processing_time_ms: None\n"
     ]
    }
   ],
   "source": [
    "gate_batch_results = process_batch_with_processor(\n",
    "    indices=indices,\n",
    "    load_func=load_func,\n",
    "    processor=blur_gate,\n",
    "    batch_size=4,\n",
    "    filter_func=None  # process all indices\n",
    ")\n",
    "print(f\"Gate results count: {len(gate_batch_results)}\")\n",
    "print(gate_batch_results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f555eb23",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned how to:\n",
    "1. List and instantiate features and gates.\n",
    "2. Run a single feature and gate on a frame.\n",
    "3. Attach results back to a `VideoFramePacket`.\n",
    "4. Use the `process_batch_with_processor` helper for batched feature/gate processing with custom loading and filtering.\n",
    "\n",
    "Adapt the `load_func` to read real frames from videos or datasets (e.g., using decord, OpenCV, or Arrow/Lance datasets)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
